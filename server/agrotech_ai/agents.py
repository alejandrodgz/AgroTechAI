"""
AI Agents for Agricultural Monitoring System
"""

import asyncio
import base64
import io
import json
import logging
import os
import time
from typing import Any, Dict

from PIL import Image

from .ollama_client import OllamaAgent, get_shared_session, reset_shared_session

# ConfiguraciÃ³n de Ollama
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
OLLAMA_GENERATE_API = f"{OLLAMA_URL}/api/generate"

# Models from environment variables with fallback defaults
MODEL_NAME = os.getenv("OLLAMA_TEXT_MODEL", "gemma3:270m")
VISION_MODEL_NAME = os.getenv("OLLAMA_VISION_MODEL", "gemma3:270m")

# Constants
UNAVAILABLE_VALUE = "No disponible"
ANALYSIS_ERROR = "Error en anÃ¡lisis"

# Configure logging
logger = logging.getLogger(__name__)


class ImageVisionAgent(OllamaAgent):
    """Agent specialized in analyzing agricultural images and providing
    detailed descriptions"""

    def __init__(self):
        super().__init__("ImageVision", "anÃ¡lisis visual de imÃ¡genes agrÃ­colas")

    def _optimize_image(self, image_base64: str) -> str:
        """Optimize image size and quality for faster processing"""
        start_time = time.time()
        logger.info(f"ðŸ–¼ï¸  [{self.role}] Starting image optimization")

        try:
            # Decode base64 image
            image_data = base64.b64decode(image_base64)
            original_size = len(image_data)
            image = Image.open(io.BytesIO(image_data))

            logger.debug(
                f"ðŸ–¼ï¸  [{self.role}] Original image: {image.size}, "
                f"format: {image.format}, size: {original_size/1024:.1f}KB"
            )

            # Resize if too large (max 1024px on longest side)
            max_size = 1024
            if max(image.size) > max_size:
                ratio = max_size / max(image.size)
                new_size = tuple(int(dim * ratio) for dim in image.size)
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                logger.info(
                    f"ðŸ–¼ï¸  [{self.role}] Image resized from {image.size} "
                    f"to {new_size}"
                )

            # Convert to RGB if necessary
            if image.mode != "RGB":
                image = image.convert("RGB")
                logger.debug(f"ðŸ–¼ï¸  [{self.role}] Image converted to RGB")

            # Save optimized image
            buffer = io.BytesIO()
            image.save(buffer, format="JPEG", quality=85, optimize=True)
            optimized_data = buffer.getvalue()
            optimized_size = len(optimized_data)

            elapsed_time = time.time() - start_time
            compression_ratio = (1 - optimized_size / original_size) * 100

            logger.info(
                "âœ… [%s] Image optimized in %.2fs: "
                "%.1fKB â†’ %.1fKB "
                "(%.1f%% reduction)",
                self.role,
                elapsed_time,
                original_size / 1024,
                optimized_size / 1024,
                compression_ratio,
            )

            return base64.b64encode(optimized_data).decode("utf-8")
        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(
                f"âŒ [{self.role}] Image optimization failed after "
                f"{elapsed_time:.2f}s: {e}"
            )
            return image_base64  # Return original if optimization fails

    async def analyze_image(self, image_base64: str) -> Dict[str, Any]:
        """Analyze agricultural image and provide detailed description
        for other agents"""
        start_time = time.time()
        logger.info(
            "ðŸ” [%s] Starting image analysis with %s", self.role, VISION_MODEL_NAME
        )

        # Optimize image first
        optimized_image = self._optimize_image(image_base64)
        prompt = """You are ImageVision, a specialized AI agent that provides \
detailed visual descriptions of agricultural images for other AI systems \
to analyze.

IMPORTANT: ALWAYS respond in SPANISH. All descriptions, recommendations, \
and text must be in Spanish.
TASK: Analyze this agricultural image and provide a comprehensive description.

IF THE IMAGE IS NOT RELATED TO AGRICULTURE, use this specific format:
{
"is_agricultural_image": false,
"reason": "Brief explanation in Spanish why the image is not relevant.",
"confidence": 0.98,
"image_description": null,
"soil_visual_indicators": null,
"environmental_context": null,
"plant_health_indicators": null,
"recommended_focus_areas": null
}

IF THE IMAGE IS RELATED TO AGRICULTURE, RESPOND ONLY WITH A JSON in this \
exact format:
{
"image_description": "Detailed visual description of the crop/plant",
"soil_visual_indicators": "Visual cues about soil condition visible in image",
"environmental_context": "Environmental factors visible in the image",
"plant_health_indicators": "Specific visual signs of plant health/disease",
"recommended_focus_areas": ["area1", "area2", "area3"],
"confidence": 0.95
}

DESCRIPTION GUIDELINES:

image_description: Comprehensive overview of what's visible \
(plants, leaves, stems, fruits, overall layout)

soil_visual_indicators: Soil color, moisture appearance, texture, \
cracking, erosion signs

environmental_context: Lighting conditions, weather signs, \
surrounding environment, growth stage

plant_health_indicators: Leaf color variations, spots, wilting, \
pest damage, growth patterns

recommended_focus_areas: Key areas that SoilSense and AgriVision \
should prioritize

confidence: Your confidence level in the description accuracy (0.0-1.0)

Focus on quantifiable visual elements (percentages, sizes, distributions) \
and use agricultural terminology.
Also focus on spanish answer for the description.

JSON:"""

        payload = {
            "model": VISION_MODEL_NAME,
            "prompt": prompt,
            "images": [optimized_image],
            "stream": False,
            "options": {
                "temperature": 0.3,
                "top_p": 0.9,
                "num_predict": 400,  # Reduced from 400
                "num_ctx": 4096,  # Context window
                "num_batch": 512,  # Batch size for processing
                "num_gpu": 0,  # Run via CPU
                "low_vram": False,  # Set to True if running out of VRAM
            },
        }
        request_timeout = 180

        logger.debug(
            "ðŸ”§ [%s] Vision payload created - Model: %s, Options: %s, "
            "Image data length: %d, Stream: %s",
            self.role,
            payload["model"],
            payload["options"],
            len(optimized_image),
            payload["stream"],
        )

        try:
            logger.info(
                "ðŸŒ [%s] Sending vision request to Ollama (timeout: %ds)",
                self.role,
                request_timeout,
            )
            logger.debug(
                "ðŸŒ [%s] Prompt length: %d chars, Image size: %d chars",
                self.role,
                len(prompt),
                len(optimized_image),
            )

            # Use asyncio to run in thread pool for better async handling
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.session.post(
                    OLLAMA_GENERATE_API, json=payload, timeout=request_timeout
                ),
            )

            elapsed_time = time.time() - start_time
            logger.info(
                "âœ… [%s] Vision analysis completed in %.2fs", self.role, elapsed_time
            )
            logger.debug(
                "ðŸŒ [%s] Response status: %s, Raw Response: %s",
                self.role,
                response.status_code,
                response.text,
            )

            result = response.json()
            resp = result.get("response", "{}")
            logger.debug(f"ðŸ“Š [{self.role}] Actual Response: {resp}")
            parsed_result = self._parse_json_response(resp)

            logger.info(f"ðŸ“Š [{self.role}] Vision response parsed successfully")
            logger.debug(f"ðŸ“Š [{self.role}] Response: {parsed_result}")

            return parsed_result
        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(
                f"âŒ [{self.role}] Vision analysis failed after "
                f"{elapsed_time:.2f}s: {e}"
            )

            # Reset shared session if connection issues persist
            if "timeout" in str(e).lower() or "connection" in str(e).lower():
                logger.warning(
                    f"ðŸ”„ [{self.role}] Resetting shared session due to "
                    f"connection issues"
                )
                reset_shared_session()
                self.session = get_shared_session()

            return self._get_fallback_response()

    def _get_fallback_response(self) -> Dict[str, Any]:
        return {
            "image_description": "Error en anÃ¡lisis de imagen",
            "soil_visual_indicators": UNAVAILABLE_VALUE,
            "environmental_context": UNAVAILABLE_VALUE,
            "plant_health_indicators": UNAVAILABLE_VALUE,
            "recommended_focus_areas": [ANALYSIS_ERROR],
            "confidence": 0.0,
        }


class AgriVisionAgent(OllamaAgent):
    """Agent specialized in visual analysis of crop conditions"""

    def __init__(self):
        super().__init__("AgriVision", "anÃ¡lisis visual de cultivos")

    async def analyze_image(self, image_description: str) -> Dict[str, Any]:
        """Analyze crop image description and provide health assessment"""
        prompt = f"""Eres AgriVision, un experto en anÃ¡lisis visual de \
cultivos agrÃ­colas.

TAREA: Analiza esta descripciÃ³n de imagen del cultivo: "{image_description}"

RESPONDE ÃšNICAMENTE CON UN JSON VÃLIDO en este formato exacto:
{{
    "crop_health": "healthy",
    "pest_detected": false,
    "leaf_condition": "good",
    "disease_probability": 0.2,
    "visual_symptoms": ["hojas verdes", "sin manchas"],
    "recommendations": ["continuar monitoreo", "revisar en 2 dÃ­as"],
    "confidence": 0.85
}}

REGLAS IMPORTANTES:
- crop_health debe ser: "healthy", "stressed", "diseased"
- pest_detected debe ser: true o false
- leaf_condition debe ser: "excellent", "good", "fair", "poor"
- disease_probability debe ser un nÃºmero entre 0.0 y 1.0
- NO agregues texto antes o despuÃ©s del JSON
- El JSON debe ser vÃ¡lido y parseable

JSON:"""

        try:
            return await self.generate_response(prompt)
        except Exception as e:
            logger.error(f"âŒ [{self.role}] Analysis failed: {e}")
            return self._get_fallback_response()

    def _get_fallback_response(self) -> Dict[str, Any]:
        return {
            "crop_health": "unknown",
            "pest_detected": False,
            "leaf_condition": "unknown",
            "disease_probability": 0.0,
            "visual_symptoms": [ANALYSIS_ERROR],
            "recommendations": ["Reintentar anÃ¡lisis"],
            "confidence": 0.0,
        }


class SoilSenseAgent(OllamaAgent):
    """Agent specialized in environmental conditions and soil analysis"""

    def __init__(self):
        super().__init__("SoilSense", "condiciones ambientales y del suelo")

    async def analyze_environment(self, conditions: str) -> Dict[str, Any]:
        """Analyze environmental conditions and soil parameters"""
        prompt = f"""Eres SoilSense, especialista en condiciones ambientales \
y del suelo para agricultura.

TAREA: Analiza estas condiciones ambientales: "{conditions}"

RESPONDE ÃšNICAMENTE CON UN JSON VÃLIDO en este formato exacto:
{{
    "soil_moisture": 45,
    "ph_level": 6.5,
    "temperature": 24,
    "humidity": 60,
    "irrigation_needed": true,
    "fertilizer_status": "adequate",
    "environmental_stress": "low",
    "alerts": ["humedad baja detectada"],
    "confidence": 0.88
}}

REGLAS IMPORTANTES:
- soil_moisture: nÃºmero entre 0-100 (porcentaje)
- ph_level: nÃºmero entre 4.0-9.0
- temperature: nÃºmero entre -10 y 50 (celsius)
- humidity: nÃºmero entre 0-100 (porcentaje)
- irrigation_needed: true o false
- fertilizer_status: "deficient", "adequate", "excess"
- environmental_stress: "low", "medium", "high"
- NO agregues texto antes o despuÃ©s del JSON

JSON:"""

        return await self.generate_response(prompt)

    def _get_fallback_response(self) -> Dict[str, Any]:
        return {
            "soil_moisture": 50,
            "ph_level": 7.0,
            "temperature": 25,
            "humidity": 50,
            "irrigation_needed": False,
            "fertilizer_status": "adequate",
            "environmental_stress": "unknown",
            "alerts": [],
            "confidence": 0.0,
        }


class CropMasterAgent(OllamaAgent):
    """Agent for integrated agricultural decision-making"""

    def __init__(self):
        super().__init__("CropMaster", "toma de decisiones agrÃ­colas integrales")

    async def make_decision(self, vision_data: Dict, soil_data: Dict) -> Dict[str, Any]:
        """Make integrated decisions based on data from multiple agents"""
        prompt = f"""Eres CropMaster, el sistema inteligente que toma \
decisiones agrÃ­colas basado en datos de mÃºltiples sensores.

DATOS DE ENTRADA:
- AgriVision: {json.dumps(vision_data, indent=2)}
- SoilSense: {json.dumps(soil_data, indent=2)}

TAREA: Fusiona toda esta informaciÃ³n y toma una decisiÃ³n integral \
sobre el manejo del cultivo.

RESPONDE ÃšNICAMENTE CON UN JSON VÃLIDO en este formato exacto:
{{
    "overall_status": "good",
    "priority_actions": [
        "regar por 10 minutos", "aplicar fungicida preventivo"
    ],
    "estimated_yield": "high",
    "risk_assessment": "low",
    "next_inspection_hours": 24,
    "economic_impact": "positive",
    "urgent_alerts": [],
    "confidence": 0.92
}}

REGLAS IMPORTANTES:
- overall_status: "excellent", "good", "warning", "critical"
- estimated_yield: "high", "medium", "low"
- risk_assessment: "low", "medium", "high", "critical"
- economic_impact: "positive", "neutral", "negative"
- next_inspection_hours: nÃºmero entre 1 y 168 (1 semana mÃ¡ximo)
- priority_actions: lista de mÃ¡ximo 4 acciones concretas
- NO agregues texto antes o despuÃ©s del JSON

JSON:"""

        return await self.generate_response(prompt)

    def _get_fallback_response(self) -> Dict[str, Any]:
        return {
            "overall_status": "unknown",
            "priority_actions": [ANALYSIS_ERROR],
            "estimated_yield": "unknown",
            "risk_assessment": "unknown",
            "next_inspection_hours": 24,
            "economic_impact": "neutral",
            "urgent_alerts": [],
            "confidence": 0.0,
        }
